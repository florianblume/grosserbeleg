\chapter{Introduction}

About three decades ago, the first minimally invasive surgery was performed. This was a huge step forward, as minimally invasive surgery offers several advantages compared to traditional surgery, such as less pain and less recovery time needed afterwards \cite{minimallyinvasive}. This kind of operation is more involved for the surgeon, as it is conducted only through a small hole in the otherwise closed abdominal wall. The executing surgeon inserts an endoscope into the patient and only sees the 2D images without any depth. Medical personnel could profit from computers assisting with augmented reality during the surgery. Next to navigation cues and other vital information, the surgical tools could be rendered into the  endoscopic video to compensate the missing depth information to some extend and facilitate the operational process. \

The task of annotating images with 3D objects is called 6D pose estimation and is already used and applied in various fields, e.g. robotics, augmented reality, medical imaging, and many more. For well-textured clearly visible objects the task is considered solved. Methods like Lowe's \cite{dglowe1} rely on detecting sparse features, for example keypoints, which are matched against a database that contains the corresponding pose.
Unfortunately, these approaches only work for objects with a strong and also visible texture. After cheap depth sensors like the Xbox Kinect became available, depth-based pose estimation procedures were able to achieve good results on texture-less but unoccluded and non-deformed objects. Many of those methods match the image against a database of templates to recover the pose. \

These techniques have in common that they are non-learning based. This means that there is no learning procedure which learns an object's appearance. Brachman et al. on the other hand used random forests and object coordinates to achieve good results on the occlusion dataset \cite{brachmann1}. The forests are trained to output the 3D coordinate on the object for every pixel. Using the 2D-3D correspondences, a robust estimate of the object's pose can be computed. With the breakthrough of neural networks in 2012 \cite{imagenet}, a lot of research in the area of learning-based methods has been spawned. The so called \textit{Convolutional Neural Networks (CNNs)} offer outstanding accuracy that beat most traditional computer vision algorithms \cite{ylecun}. Krull et al. \cite{akrull} combine the idea of learning the 3D coordinates representation of an object with the power of CNNs. A major drawbacks of neural networks is the need for training data. In contrast to earlier methods, the data needs to be present upfront and annotated with the desired output. \

The goal of this work is to analyze the process of manual 6D pose estimation and develop a tool based on the carved out requirements. The tool is supposed to enable annotating the medical images, similar to the ones provided with the project. 

To support the user in the still tedious and time-consuming task of annotating images, a neural network is manufactured for estimating the objects' poses. The base architecture of the network is \textit{ResNet} \cite{resnet}. ResNet is a network presented in 2015, which allows for deeper networks by adding the input to intermediate layers and still achieves state-of-the-arts results. Similarily to Brachmann et al., the network predicts object coordinates. But since segmentation images are already provided by the medical data, the focus is only on coordinate prediction and not instance segmentation. The network works on RGB-only images because videos from endoscopes often lack depth information. \

The remainder of the work is structured as follows: Chapter \ref{chapter:background} explains the basic concepts of deep learning and pose estimation. Chapter \ref{chapter:related_work} then introduces and discusses the latest research in the area of pose estimation, after giving a brief overview over earlier methods. The process of manually annotating images with 6D poses of 3D models with the aid of the developed tool, is laid out in chapter \ref{chapter:image_pose_annotation}. Chapter \ref{chapter:semi_automatic} describes the workflow of annotating images with the aid of the neural network. The datasets used to train the network as well as the respective experiments are presented in chapter \ref{chapter:experiments}. Last but not least, chapter \ref{chapter:conclusions} summarizes this work, draws conclusions and gives a prospect into possible future research.