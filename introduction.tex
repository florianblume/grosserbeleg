\chapter{Introduction}

About three decades ago, the first minimally invasive surgery was performed. This was a huge step forward, as minimally invasive surgery offers several advantages compared to traditional surgery, such as less pain for the patient and less recovery time needed afterwards \cite{minimallyinvasive}. This kind of operation is more involved for the surgeon, as it is conducted only through a small hole in the otherwise closed abdominal wall. The executing surgeon inserts an endoscope into the patient and only sees the 2D images without any depth. Medical personnel could profit from computers assisting with augmented reality during the surgery. Next to navigation cues and other vital information, the surgical tools could be rendered into the  endoscopic video to compensate the missing depth to some extend and facilitate the operational process. \

This task of annotating images with 3D objects is called 6D pose estimation and is already used and applied in various fields, e.g. robotics, augmented reality, medical imaging, and many more. There exist various ways to recover the 6D poses on an image. 

For well-textured clearly visible objects the task is considered solved. Methods like \cite{dglowe1} by Lowe \etal rely on detecting sparse features, for example keypoints, which are matched against a database that contains the corresponding pose.
Unfortunately, these approaches only work for objects with a strong and also visible texture. After cheap depth sensors like the Xbox Kinect became available, depth-based pose estimation procedures were able to achieve good results on texture-less but unoccluded and non-deformed objects. Many of those methods match the image against a database of templates to recover the pose. 

These techniques have in common that they are non-learning based. This means that there is no learning procedure which learns an object's appearance. Brachman et al. on the other hand used random forests and object coordinates to achieve very good results on the occlusion dataset \cite{brachmann1}. The forests are trained to output for every pixel the 3D location on the object. Using the 2D-3D correspondences, a robust estimate of the object's pose can be computed. Krull et al. \cite{akrull} combine the idea of learning the 3D coordinates representation of an object with the power of the so called \textit{Convolutional Neural Networks (\gls{cnn}s)}. \gls{cnn}s became popular around 2012, after training deeper networks turned out to be achievable in a feasible time on graphics cards. Deep \gls{cnn}s offer outstanding accuracy that beat most traditional computer vision algorithms \cite{ylecun}. A drawback of \gls{cnn}s is their need for training data, which has to be annotated with the desired network output beforehand.

The goal of this work is to create a system that allows users to annotate large datasets, effectively and efficiently. We will introduce a tool that allows to annotate images with the 6D poses of arbitrary objects. To further reduce the time needed to annotate a whole dataset, we also present a neural network to support the user in the annotation process. The base architecture of the network is \textit{ResNet} \cite{resnet}. ResNet is a network presented in 2015, which allows for deeper networks and still achieves state-of-the-art results. The network is tailored to the characteristics of the dataset of the \textit{Endoscopic Vision Challenge} and uses images and the corresponding segmentation masks to predict object coordinates. We examined multiple configurations to obtain the best network and connected the network to the program.

The remainder of the work is structured as follows: Chapter \ref{chapter:background} explains the basic concepts of deep learning and pose estimation. Chapter \ref{chapter:related_work} then introduces and discusses the latest research in the area of pose estimation, after giving a brief overview over earlier methods. The process of manually annotating images with 6D poses of 3D models with the aid of the developed tool, is laid out in Chapter \ref{chapter:manual_annotation}. Chapter \ref{chapter:semi_automatic} describes the workflow of annotating images with the aid of the neural network. The datasets used to train the network as well as the respective experiments are presented in Chapter \ref{chapter:experiments}. Last but not least, Chapter \ref{chapter:conclusions} summarizes this work, draws conclusions and gives a prospect into possible future research.