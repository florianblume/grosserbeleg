\chapter{Introduction}

About three decades ago, the first minimally invasive surgery was performed. This was a huge step forward, as minimally invasive surgery offers several advantages compared to traditional surgery, such as less pain for the patient and less recovery time needed afterwards \cite{minimallyinvasive}. The surgeon conducts the operation only through a small hole in the otherwise closed abdominal wall. This makes it more involved than former procedures, of course. The executing surgeon inserts an endoscope into the patient and only sees the 2D images without any depth. Medical personnel could profit from computers assisting with augmented reality. Next to navigation cues and other vital information, such an assistance system could render the surgical tools into the  endoscopic videos to compensate the missing depth to some extend and facilitate the operational process. \

This task of automatically determining the location and rotation of 3D objects on an image is called 6D pose estimation. Robotics, augmented reality and medical imaging already apply 6D pose estimation, to name just a few examples. There exist various ways to recover the 6D poses on an image. 

For well-textured clearly visible objects the task is considered solved. Methods like \cite{dglowe1} by Lowe \etal rely on detecting sparse features, for example keypoints, which are matched against a database that contains the corresponding pose.
Unfortunately, these approaches only work for objects with a strong and also visible texture. After cheap depth sensors like the Xbox Kinect became available, pose estimation procedures relying on depth were able to achieve good results on texture-less but unoccluded and non-deformed objects. 

Many of the mentioned techniques have in common that they are not learning-based. This means that there is no learning procedure which learns an object's appearance. Brachman et al. on the other hand used random forests  to achieve very good results for occluded objects \cite{brachmann1}. The forests are trained to output the 3D location on the object for every pixel. Using the 2D-3D correspondences, a robust estimate of the object's pose can be computed. Krull et al. \cite{akrull} combine the idea of learning to predict the 3D coordinates  with the power of so called \textit{Convolutional Neural Networks (CNNs)}. CNNs became popular around 2012, after training deeper networks turned out to be achievable in a feasible time on graphics cards. Deep CNNs offer outstanding accuracy that beat most traditional computer vision algorithms \cite{ylecun}. A drawback is their need for training data. Images have to be annotated with the desired network output beforehand.

The goal of this work is to create a system that allows users to annotate large datasets, effectively and efficiently. We will introduce a tool that allows to annotate images with the 6D poses of arbitrary objects. To further reduce the time needed to annotate a whole dataset, we also present a neural network to support the user in the annotation process. The base architecture of the network is \textit{ResNet} \cite{resnet}. ResNet is a network presented in 2015, which allows construction of deeper networks and outperformed all previous architectures this way. The network is tailored to the characteristics of the dataset of the \textit{Endoscopic Vision Challenge} \cite{endovis}, this means that it uses images and the corresponding segmentation masks to predict object coordinates. We also examine multiple configurations to obtain the best network.

The remainder of the work is structured as follows: Chapter \ref{chapter:background} explains the basic concepts of deep learning and pose estimation. Chapter \ref{chapter:related_work} introduces and discusses the latest research in the area of pose estimation, after giving a brief overview over earlier methods. We present the developed tool in Chapter \ref{chapter:manual_annotation}. Chapter \ref{chapter:semi_automatic} describes the workflow of annotating images with the aid of the neural network. In Chapter \ref{chapter:experiments}, we explain the datasets used for the experiments and discuss the latter in depth. The last chapter summarizes this work, draws conclusions and gives a prospect into possible future research.